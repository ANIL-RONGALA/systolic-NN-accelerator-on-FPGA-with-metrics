# ğŸ§® Systolic Neural-Network Accelerator on FPGA (with Metrics)
---
A hardware-accelerated matrix-multiplication engine for neural networks, implemented in SystemVerilog, verified with a Python golden model, and synthesised on FPGA with performance metrics.

## ğŸ“˜ 1. Project Overview
---
This project presents the design, implementation, and evaluation of a scalable systolic-array neural-network accelerator. Key features:

A 4Ã—4 Processing-Element (PE) array as the base unit

Tile-based matrix multiplication enabling 64Ã—64 matrix support

Verification via a Python golden model and SystemVerilog testbench

FPGA synthesis (Quartus/Vivado) and extraction of performance metrics (resource usage, frequency, throughput)

Modular architecture supporting expansion to larger arrays or other NN primitives

This work is ideal for digital-design, computer-architecture and AI-hardware research, and is fully reproducible with RTL + testbench + golden model.

## ğŸ§° 2. Repository Contents
---
```
systolic-NN-accelerator-on-FPGA-with-metrics/
â”‚
â”œâ”€â”€ golden/                  # Python golden model + test-vector generator  
â”‚   â”œâ”€â”€ golden_matmul.py  
â”‚   â”œâ”€â”€ generate_vectors.py  
â”‚   â””â”€â”€ vectors/             # input/output matrices  
â”‚       â”œâ”€â”€ A.txt  
â”‚       â”œâ”€â”€ B.txt  
â”‚       â””â”€â”€ C.txt  
â”‚
â”œâ”€â”€ rtl/                     # RTL implementation in Verilog/SystemVerilog  
â”‚   â”œâ”€â”€ pe.v  
â”‚   â”œâ”€â”€ pe_array.v  
â”‚   â”œâ”€â”€ controller.v  
â”‚   â”œâ”€â”€ memory.v  
â”‚   â””â”€â”€ top.v  
â”‚
â”œâ”€â”€ tb/                      # Testbenches in SystemVerilog  
â”‚   â”œâ”€â”€ tb_pe.sv  
â”‚   â”œâ”€â”€ tb_array.sv  
â”‚   â”œâ”€â”€ tb_top.sv  
â”‚   â””â”€â”€ run.do                # ModelSim simulation script  
â”‚
```
<!--
â”œâ”€â”€ quartus_project/         # FPGA synthesis project (Quartus/Vivado)  
â”‚   â”œâ”€â”€ project.qpf  
â”‚   â”œâ”€â”€ project.qsf  
â”‚   â””â”€â”€ reports/              # synthesis & timing reports  
â”‚
â”œâ”€â”€ docs/                    # Documentation  
â”‚   â”œâ”€â”€ design_doc.pdf  
â”‚   â”œâ”€â”€ block_diagram.png  
â”‚   â””â”€â”€ final_paper.pdf  
â”‚
-->
```
â”œâ”€â”€ LICENSE                  # MIT License  
â””â”€â”€ README.md                # This file  
```
## ğŸ” 3. Key Design Features
---
Processing Element (PE): Performs multiply-accumulate (MAC) operations; forms the building block of the systolic array.

Systolic Array (4Ã—4): Enables spatial and temporal reuse of data, reducing memory bandwidth and increasing throughput.

Tile-Based Matrix Multiplication: Supports larger matrices (e.g., 64Ã—64) by tiling through the 4Ã—4 array.

Python Golden Model: Verifies functional correctness of the design by comparing RTL outputs to high-level model results.

FPGA Metrics Collection: Resource usage (LUTs/FFs/BRAM), maximum frequency, throughput (MACs/sec) and power estimates are reported.

Modular and Scalable Architecture: Easier to scale to larger arrays (8Ã—8, 16Ã—16) for future work.

## ğŸ§ª 4. How to Build and Simulate
---
Simulation (Golden + RTL)

Run golden_matmul.py to generate input-output vectors.

Launch ModelSim/Questa with tb_top.sv and run.do.

Compare RTL output C.txt results with golden model.

Synthesis (FPGA)

Open the quartus_project (or equivalent Vivado) folder.

Assign FPGA device and compile.

Examine reports/ for resource usage, critical path, and timing.

Run post-place-and-route simulation or hardware test if board available.

## ğŸ“Š 5. Metrics & Results
---
Functional parity verified between golden model and RTL for tile sizes up to 64Ã—64.

Example reporting:

LUTs: ~X, FFs: ~Y, BRAM: ~Z (for 4Ã—4 array)

Maximum frequency: ~F MHz

Throughput: ~T GMAC/s

(Refer to reports/ for full details.)

## ğŸ“‚ 6. How to Reproduce or Extend
---
Fork or clone this repository.

Modify the PE or array dimensions (e.g., change to 8Ã—8).

Update generate_vectors.py for new matrix sizes.

Add new testbenches or extend existing ones for new functionality (e.g., activation functions).

Re-synthesise and collect new metrics.

You may integrate this accelerator into a larger SoC or pipeline for inference tasks.

## ğŸš€ 7. Future Scope & Research Extensions
---
Extend array size (8Ã—8, 16Ã—16, 32Ã—32) for larger NN workloads.

Integrate activation functions (ReLU, Sigmoid, Quantization) within PEs.

Add support for sparse matrix formats or mixed-precision (INT8, FP16) for efficiency.

Implement dynamic reconfiguration: switch tile sizes at runtime.

Incorporate AI/ML workloads: convolutional layers, transformer accelerators.

Design full SoC with on-chip memory, DMA engine, external interface (PCIe, AXI).

Port to ASIC for research on power/area optimization, enabling PhD-level publications.

## ğŸ“ 8. License
---
This project is released under the MIT License â€” see LICENSE file for details.

ğŸ”§ Acknowledgments

This work builds on standard systolic-array architecture concepts from literature, and extends them with a reproducible FPGA/RTL/C verification stack.

Note:
A portion of documentation formatting and organization was enhanced using AI tools for clarity. The architecture, logic design, RTL implementation, verification, and metric collection are original.
